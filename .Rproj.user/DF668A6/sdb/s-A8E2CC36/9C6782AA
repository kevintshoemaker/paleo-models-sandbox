{
    "collab_server" : "",
    "contents" : "##########################\n# This master script performs Approximate Bayesian Analysis\n#     algorithms and goodness-of-fit testing for the Paleo simulations\n#         USES THE 'ABC' PACKAGE (for post-hoc elimination-style ABC)\n#\n#  Authors: Kevin Shoemaker and Damien Fordham\n# \n#  22 May 2017 -- started scripting\n#\n##########################\n\nrm(list=ls())\n\n####################\n# LOAD PACKAGES\n###################\n\n##ABC code\nlibrary(abc)\nlibrary(raster)\n#library(ggmap)\nlibrary(mapproj)\nlibrary(rworldmap)\nlibrary(abctools)    # for fancy calibration of the posterior - for later? \n\n\n####################\n# LOAD FUNCTIONS\n####################\n\n# prepare data for ABC\n\nprepareForABC <- function(data=all,subset=c(1,3),standardized=T){\n  par.sim <<- data[,estimable.params]     # samples run from parameter space \n  if(standardized){ \n    maxstats <- apply(data[,test.statistics[subset],drop=FALSE],2,function(t) max(t,na.rm=T))\n    for(i in test.statistics[subset]){\n      data[,i] <- data[,i]/maxstats[i]\n    }\n    stat.obs <<- test.statistics.obs[test.statistics[subset]]/maxstats\n  } else{\n    stat.obs <<- test.statistics.obs[test.statistics[subset]]\n  }\n  stat.sim <<- data[,test.statistics[subset],drop=FALSE]\n}\n\n\n## plot priors for all parameters (to check for uniformity) \n\nvisualize.priors <- function(){\n  par(mfrow=c(2, 3),ask=TRUE)\n  \n  for(i in 1:length(estimable.params)){\n    hist(all[,estimable.params[i]], breaks = 50,main=estimable.params[i])\n  }\n}\n\n\n## note: this function only works with rejection method...\nvisualize.posteriors <- function(){\n  par(mfrow=c(2, 3),ask=TRUE)\n  posterior_ndx <- as.numeric(rownames(as.matrix(rej$ss))) \n  all_post <- all[posterior_ndx,]\n  for(i in 1:length(estimable.params)){\n    hist(all_post[,estimable.params[i]], breaks = 50,main=estimable.params[i],freq=FALSE)\n    lines(density(all_post[,estimable.params[i]]),col=\"blue\",lwd=2)\n  }\n}\n\n\n\nvisualize <- function(target=target){\n  \n  par(ask=TRUE)\n  \n  #if(toPlot==\"ABUNDANCE\"){\n    matches <- as.numeric(unlist(regmatches(target, gregexpr(\"[[:digit:]]+\", target))))\n    nichebreadth <- matches[1]\n    sample <- matches[2]\n    \n    \n    filename <- sprintf(\"NicheBreadth%i_TotAbund_output_alldata.csv\",nichebreadth)\n    \n    setwd(SIM_RESULTS_DIR)\n    \n    #getwd()\n    #list.files()\n    \n    \n    abund <- read.csv(filename, header = T,stringsAsFactors = FALSE)    # if(!\"abund\"%in%ls(name=.GlobalEnv))  \n  \n    rownames(abund) <- abund[,1]\n  \n    #head(abund[,2000:2010])\n    \n    time<- rev(seq(from =0, to =80000, by = 25))\n    \n    timeall <- time[2361:2928]\n    timebreak1 <- time[2708:2928]\n    timebreak2 <- time[2361:2707]\n    \n    ## Provide row id of simulation of interest\n    abundall <- as.numeric(abund[target,2362:2929])     \n    abundbreak1 <- as.numeric(abund[target,2709:2929])\n    abundbreak2 <- as.numeric(abund[target,2362:2708])\n    \n    #graphics.off()\n    plot(timeall, abundall, pch = 16, cex = 0.2, col = \"blue\", main = \"Change in Ne\", \n         ylim=c(0,min(2000000,max(abundall+10000))),xlab = \"Time (year)\", ylab = \"Abundance\")   #  ,\n    \n    break1coef <- round(lm(abundbreak1~timebreak1)$coefficients[2],2) \n    break2coef <- round(lm(abundbreak2~timebreak2)$coefficients[2],2)\n    \n    abline(lm(abundbreak1~timebreak1))\n    abline(lm(abundbreak2~timebreak2))\n    \n    \n    ## Extinction presumed to have occurred by 3000 years ago; and only interested in records after 21,000 years ago\n    time2 <- (eff_popsize[61:420,1])\n    abund2 <-(eff_popsize[61:420,2])\n    data2 <- cbind(time2, abund2)\n    \n    points(time2, abund2, cex = 0.1, col = \"red\",pch=20)\n  #}\n  \n  \n  #if(toPlot==\"EXTINCTION_DATE\"){\n    matches <- as.numeric(unlist(regmatches(target, gregexpr(\"[[:digit:]]+\", target))))\n    nichebreadth <- matches[1]\n    sample <- matches[2]\n    \n    colors <- c(\"red\",\"orange\",\"yellow\",\"green\",\"blue\",\"purple\",\"violet\",\"black\")\n    cutoffs <- c(100000,70000,40000,20000,15000,10000,5000,4000,0)\n    \n    #colname <- sprintf(\"NicheBreadth%i_LHS_Sample%i.mp\",nichebreadth,sample)\n    \n    filename <- sprintf(\"NicheBreadth%i_FinalYear_output_alldata_formatted.csv\",nichebreadth)\n    \n    setwd(SIM_RESULTS_DIR)\n    \n    extinct <- read.csv(filename, header = T,stringsAsFactors = FALSE)    # if(!\"abund\"%in%ls(name=.GlobalEnv))  \n    \n    #rownames(extinct) <-  \n    \n    xcords <- extinct[,1]\n    ycords <- extinct[,2]\n    head(extinct[,1:10])\n    \n    if(any(xcords<0)) xcords[which(xcords<0)] <- 180+(180 - abs(xcords[which(xcords<0)]))\n    \n    thissample <- extinct[,target]\n    \n    time<- rev(seq(from =0, to =80000, by = 25))\n    \n    ext.time <- time[thissample]\n    \n    ext.time.cols <- 9-as.numeric(cut(ext.time,cutoffs))\n    \n    #exttime_scaled <- ext.time/80000\n    \n    cols <- colors[ext.time.cols] #colorRamp(c(\"blue\",\"red\"), bias = 1, space = c(\"rgb\"),\n             #         interpolate = c(\"linear\"), alpha = FALSE)(exttime_scaled)\n    \n    #graphics.off()\n    #map <- get_map(location = 'Asia', zoom = 3)\n    newmap <- getMap(resolution = \"low\")\n    #ggmap(map)\n    \n    \n    \n    par(mfrow=c(2,1))\n    plot(newmap,xlim = c(20, 200),\n         ylim = c(30, 80),\n         asp = 1,\n         main=\"simulated\"\n    )\n    points(xcords,ycords,pch=20,cex=0.01,col=cols)\n    \n    ### location of final extinction\n    ndx <- which(ext.time==min(ext.time,na.rm=T))\n    xcords_lastext <- extinct[ndx,1]\n    \n    if(any(xcords_lastext<0)) xcords_lastext[which(xcords_lastext<0)] <- 180+(180 - abs(xcords_lastext[which(xcords_lastext<0)]))\n    \n    ycoord_lastext <- extinct[ndx,2]\n    \n    # plot(newmap,xlim = c(50, 219),\n    #      ylim = c(30, 71),\n    #      asp = 1,\n    #      main=\"real\"\n    # )\n    points(xcords_lastext,ycoord_lastext,pch=\"X\",cex=2,col=\"black\")\n    \n    \n    plot(newmap,xlim = c(20, 200),\n         ylim = c(30, 80),\n         asp = 1,\n         main=\"real\"\n    )\n    \n    ext.time.cols <- 9-as.numeric(cut(extinct_date$Ext,cutoffs))\n    \n    #exttime_scaled <- ext.time/80000\n    \n    cols <- colors[ext.time.cols]\n    #cols <- colorRamp(c(\"blue\",\"red\"), bias = 1, space = c(\"rgb\"),\n     #                 interpolate = c(\"linear\"), alpha = FALSE)(extinct_date$Ext/80000)\n\n    points(extinct_date$Mammoth_estimates.CI.Barnoski._Long,extinct_date$Mammoth_estimates.CI.Barnoski._Lat,pch=20,cex=0.01,col=cols)    \n    \n\n    points(last.locations[5,],pch=\"X\",cex=2,col=\"yellow\")\n    \n    \n    \n    \n  #}\n  #graphics.off()\n  \n}\n\n\n####################\n# LOAD SIMULATIONs\n#\n# specify directory that contains all simulation results and parameter values, prepared for ABC\n###################\n\nKEVIN_LAB <- TRUE\nDAMIEN <- FALSE \n\nif(KEVIN_LAB) BASE_RESULTS_DIR <- \"E:\\\\MammothResults\"\n\nSIM_RESULTS_DIR <- sprintf(\"%s\\\\ABCAnalysisInputs\\\\\",BASE_RESULTS_DIR)\nTARGETS_DIR <- sprintf(\"%s\\\\ABCAnalysisTargets\",BASE_RESULTS_DIR)\nGENETIC_TARGETS_DIR <- sprintf(\"%s\\\\Genetic\",TARGETS_DIR)\nFOSSIL_TARGETS_DIR <- sprintf(\"%s\\\\Fossil\",TARGETS_DIR)   \nINPUTS_DIR <- sprintf(\"%s\\\\ABCAnalysis\\\\MergedInputs\",BASE_RESULTS_DIR)\n\nsetwd(INPUTS_DIR)\n\n\ndat1 <- read.csv(\"NicheBreadth40_ABC_data_revised.csv\", header = T,stringsAsFactors = FALSE)\ndat2 <- read.csv(\"NicheBreadth50_ABC_data_revised.csv\", header = T,stringsAsFactors = FALSE)\ndat3 <- read.csv(\"NicheBreadth60_ABC_data_revised.csv\", header = T,stringsAsFactors = FALSE)\ndat4 <- read.csv(\"NicheBreadth70_ABC_data_revised.csv\", header = T,stringsAsFactors = FALSE)\ndat5 <- read.csv(\"NicheBreadth80_ABC_data_revised.csv\", header = T,stringsAsFactors = FALSE)\ndat6 <- read.csv(\"NicheBreadth90_ABC_data_revised.csv\", header = T,stringsAsFactors = FALSE)\n\nall <- rbind(dat1,dat2,dat3,dat4,dat5,dat6)\n\nnrow(all)   # 74931 simulations\n\nhead(all)   # check the combined dataframe \n\n## Generate a version with no NA values (some analyses do not allow NA values)\n\nall_nona <- na.omit(all)   ### remove all na observations\n\n\n### Generate a version with no persistent simulations (remove all sims that never went extinct or extinct in first time step\n\nkeep <- !is.na(all$extinction_yrs)\nlength(which(keep))   # 20126 simulations remaining\nnrow(all)-length(which(keep))   # 54805 sims did not undergo global extinction within the simulation period\n\nall_extinct <- all[keep,]     # remove the ca. 55k simulations that were not plausible.\n\n\n  # explore any remaining NAs in the data frame, after removal of the non-extinctions...\nremaining.nas <- all_extinct[which(is.na(all_extinct),arr.ind=TRUE)[,1],]\n\nnrow(remaining.nas)     # 240 simulations with NAs for dist_centroid and dist_fossil\nremaining.nas$dist_fossil\nremaining.nas$dist_centroid\ntail(remaining.nas)\n\n\n\n\n\n####################\n# LOAD REAL-WORLD DATA \n\n# targets for matching with simulated data- for ABC\n####################\n\nsetwd(GENETIC_TARGETS_DIR)\n  \n    # load data on effective population size from genetic data\neff_popsize <- read.csv(\"Mammoth_Ne.csv\", header = T)\n\n\nsetwd(FOSSIL_TARGETS_DIR)\n\n  # load data on extinction date from the fossil record, only fossil locations... INTERPOLATED\nextinct_date <- read.csv(\"Barnoski_matched_estimates_fossils.csv\", header = T)\n\nextinct_date$Mammoth_estimates.CI.Barnoski._Long[which(extinct_date$Mammoth_estimates.CI.Barnoski._Long<0)]<- 180+(180 - abs(extinct_date$Mammoth_estimates.CI.Barnoski._Long[which(extinct_date$Mammoth_estimates.CI.Barnoski._Long<0)]))\n\n\n  # Generate a version that has extinction date only for sites with fossil records (not interpolated)\nextinct_date_fossilsites <- extinct_date[extinct_date$Fossil.Data==1,]\n\n     # final locations of extinction, observed\nlast.locations <- extinct_date[which(extinct_date$Ext==min(extinct_date$Ext)),c(1,2)]\n\nnames(last.locations) <- c(\"X\",\"Y\")\n\nhist(extinct_date$Ext)\n\n####################\n# OBSERVED STATISTICS FOR MATCHING WITH SIMULATION RESULTS\n###################\n\ntest.statistics.obs <- c(64.0765, 11.4448, 0, 0, 0, 0)\n\nnames(test.statistics.obs) <- c(\"abundcoef1\",\"abundcoef2\",\"extinctpattern\",\"extinction_yrs\",\"dist_centroid\",\"dist_fossil\")\n\ntest.statistics.obs\n\n####################\n# SET UP VARIABLEs FOR ABC\n\n# All variable (estimable) parameters and all summary statistics for matching with data\n###################\n\n      #### What is the multivariate parameter space we are attempting to estimate?\n\nestimable.params <- c(\n  \"RMAX\",      \n  \"SD\",        # env. stochasticity\n  \"ALLEE\",     \n  \"DENSITY\",   # max density of mammoths\n  \"DISP1\",     # determines rate of staying/leaving a given cell\n  \"DISP2\",     # max dispersal distance\n  \"HARV\",      # maximal harvest rate\n  \"HARVZ\",     # regulates shape of functional response\n  \"HUMAN\",\n  \"summ_precip_median\",\n  \"summ_precip_range\",\n  \"jan_temp_median\",\n  \"jan_temp_range\",\n  \"jul_temp_median\",\n  \"jul_temp_range\"\n)\n\nestimable.params\n\n   #### Which test statistics should we use? \n\ntest.statistics <- c(\n  \"abundcoef1\",    # slope of abundance decline, most recent\n  \"abundcoef2\",     # slope of abuncance decline, older\n  \"extinctpattern\",\n  \"extinction_yrs\",\n  \"dist_centroid\",\n  \"dist_fossil\"\n)\n\ntest.statistics\n\n\n\n### NOTES: which summary statistics to use??\n\n## use just coefficient 1 (abundance by itself) \n## timing of extinction is critical\n## play around with both distances but don't use both...\n## pattern by itself\n## pattern with extinction?   bring NAs in?\n\n\n#####################\n# PERFORM BASIC VISUALIZATIONS AND DATA CHECKS\n#####################\n \n### VISUALIZE TEST STATISTICS  (with targets overlaid)\n\npar(mfrow=c(2,3))\nfor(i in test.statistics){\n  hist(all[,i],main=i)\n  abline(v=test.statistics.obs[i],col=\"green\",lwd=2)\n}\n\n\n### VISUALIZE PRIORS (check for uniformity- don't need to run this every time)\n\nvisualize.priors()\n\n####################\n# SELECT SUMMARY STATISTICS\n####################\n\n?selectsumm\n\nprepareForABC(all_nona,subset=c(1:6),standardized = TRUE)\n\nselectstats <- abctools::selectsumm(test.statistics.obs, par.sim, \n                                     stat.sim, ssmethod =mincrit,  # AS.select,    #  \n                                     final.dens = FALSE)\n\n# which summary statistics were selected by the \"selectsumm\" algorithm?   # # best subset is variables 1 and 3?\n\ncolnames(selectstats$best) <- test.statistics\n\nselectstats$best\n\n#### make final selections...\n\ntest.statistics\n\n\n#prepareForABC(all,subset=c(1,2,4,6))\n\n\n   ## here we subset the targets to the best set identified above\nprepareForABC(all,subset=c(5,6),standardized=T)\n\n\nhead(par.sim)\nhead(stat.sim)   # make sure the right summary statistics are being used\nstat.obs\n\n\n#####################\n# PERFORM PRELIMINARY ABC, VISUALIZATIONS AND DATA CHECKS\n#####################\n\n### Using the rejection Method to find the best fits\n\n\n#####\n# ABC analysis\n#####\nrej <- abc(target=stat.obs, param=par.sim, sumstat=stat.sim, tol=.01, \n           method =\"neuralnet\", numnet=20, sizenet=10,trace=T,maxit=1000,lambda = c(0.0001,0.001,0.01))   # rejection  # loclinear\n\n# NOTE: neural net method seems to perform the best...\n\n   # explore the abc object...\nhead(rej$adj.values)\nhead(rej$unadj.values)\nrej$weights\n\nrej$dist    # this is the euclidean distance between simulation results and observations \n\n######     \n # summary and diagnostic plots\n######\n\nplot(rej,param=par.sim)     # visualize all priors and posteriors\n\nrej$ss  # all simulation runs that were kept for the posterior\n\n    ### credible intervals\nsummary(rej)\n\n\n\n    ### cross validation (can take a while with neural net)\n\n### the cross validation does NOT use the observed targets. Instead, it takes simulated data, one at a time, as\n#    the \"observed\" targets, and uses the remaining simulations to perform an ABC analysis with the new\n#    \"observed\" summary statistics. \n#    this way, we can look at the overall quality of the ABC (ability to estimate selected parameters) \n#    if the algorithm is good (summary stats are informative) then the parameters estimated by ABC should\n#    match the parameters of the simulation that in fact generated the psuedo-observed data... \n\ncv.res.reg <- cv4abc(data.frame(Density=par.sim$DENSITY,Allee=par.sim$ALLEE,Jultemp=par.sim$jul_temp_median), stat.sim,\n                     nval=20, tols=c(0.01,0.05),   # method=\"rejection\")\n                     method =\"neuralnet\", numnet=10, sizenet=10,trace=F,maxit=500,lambda = c(0.0001,0.001,0.01)) \nplot(cv.res.reg)\n\nsummary(cv.res.reg)   # good summary of model performance\n\n\n\n#############################\n# VISUALIZE ABUNDANCE TRAJECTORIES AND EXTINCTION TIMES\n#############################\n\n### find the best simulations to explore further\n\nif(length(test.statistics)>1){\n  ndx <- as.numeric(rownames(rej$ss))\n  ranking <- order(apply(as.matrix(rej$ss,min),1,sum))\n  ndx <- ndx[ranking]\n  ndx_min <- as.numeric(names(which.min(apply(as.matrix(rej$ss,min),1,sum))))\n}else{\n  ndx <- as.numeric(names(rej$ss))\n  ranking <- order(as.vector(rej$ss))\n  ndx <- ndx[ranking]\n  #ndx_min <- as.numeric(names(which(rej$ss == min(rej$ss), arr.ind = TRUE)))[1]\n  #mins <- apply(as.matrix(rej$ss,min),2,min)\n}\n\n\nindex <- ndx[1]   # one of a set of all models used in the posterior\nindex <- ndx_min   # the best fit model\n\ntarget <- all[index,]$model\n\n#toPlot <-  \"EXTINCTION_DATE\"  # \"ABUNDANCE\"     # \n\n\n### NOTE on color scheme: red is >70k time to extinction. \n                      # orange is from 40 to 70k\n                      # yellow is from 20 to 40k\n                      # green is from 15 to 20k\n                      # blue is from 10 to 15k\n                      # purple is from 5 to 10k\n                      # indigo is from 4-5k\n                      # black is from 3-4k\n\ngraphics.off()\nvisualize(target)\n\n\n\n#####################\n# VISUALIZE POSTERIORS\n#####################\n\n\n    # only works for elimination method\n # visualize.posteriors()\n\n\n\n#####################\n# from ABC package\n\n## Note: cv4postpr is only for model selection...\n          # also \"postpr\" is only for model selection\n          # use \"cv4abc\" function for our purposes\n\n\n## Note: the neural network method seems to be the best- local linear does not work at all!\n\n\n##### TESTS\n\n# does removing nas do anything to the results? it shouldn't!\n# does standardizing the targets have an effect?\n\n\n",
    "created" : 1495475778011.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4194665925",
    "id" : "9C6782AA",
    "lastKnownWriteTime" : 1495838124,
    "last_content_update" : 1495838124702,
    "path" : "E:/GIT/paleo-models-sandbox/ABC_Mammoth_Code.R",
    "project_path" : "ABC_Mammoth_Code.R",
    "properties" : {
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}
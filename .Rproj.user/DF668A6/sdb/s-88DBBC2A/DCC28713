{
    "collab_server" : "",
    "contents" : "##########################\n# This master script performs Approximate Bayesian Analysis\n#     algorithms and goodness-of-fit testing for the Paleo simulations\n#         USES THE 'ABC' PACKAGE (for post-hoc elimination-style ABC)\n#\n#  Authors: Kevin Shoemaker and Damien Fordham\n# \n#  22 May 2017 -- started scripting\n#\n##########################\n\n#######################\n#   CLEAR THE WORKSPACE\n#######################\n\nrm(list=ls())\n\n#########################\n#   PRELIMINARY: SET PROJECT DIRECTORIES\n#########################\n\nKEVIN = FALSE # FALSE  # TRUE\nDAMIEN = FALSE\nHRA_LAB = FALSE # TRUE\nKEVIN_LAB = TRUE\n\n####################\n#   PRELIMINARY: LOAD FUNCTIONS\n#################### \n\nif(KEVIN) CODE_DIRECTORY <- \"C:\\\\Users\\\\Kevin\\\\GIT\\\\paleo-models-sandbox\"       # code directory should be your local copy of the GitHub repository   \nif(HRA_LAB) CODE_DIRECTORY <- \"C:\\\\Users\\\\Akcakaya\\\\Desktop\\\\Mammoth Model\\\\paleo-models-sandbox\"\nif(DAMIEN) CODE_DIRECTORY <-  \"C:\\\\Users\\\\Damien Fordham\\\\Documents\\\\GitHub\\\\paleo-models-sandbox\"\nif(KEVIN_LAB) CODE_DIRECTORY <- \"E:\\\\GIT\\\\paleo-models-sandbox\"\n\nsetwd(CODE_DIRECTORY)\nsource(\"Rfunctions_PALEO_UTILITY.r\")     # Load all functions for Paleo project\n\n####################\n#  PRELIMINARY: SET UP WORKSPACE AND LOAD PACKAGES\n####################\n\nSetUpWorkspace()   # function loads packages and sets up the workspace...  \n\n####################\n# LOAD ADDITIONAL PACKAGES\n###################\n\n##ABC code\nlibrary(abc)\nlibrary(raster)\n#library(ggmap)\nlibrary(mapproj)\nlibrary(rworldmap)\nlibrary(abctools)    # for fancy calibration of the posterior - for later? \n\n\n####################\n# LOAD FUNCTIONS\n####################\n\n# prepare data for ABC\n\nprepareForABC <- function(data=all,subset=c(1,3),standardized=T){\n  par.sim <<- data[,estimable.params]     # samples run from parameter space \n  if(standardized){ \n    maxstats <- apply(data[,test.statistics[subset],drop=FALSE],2,function(t) max(t,na.rm=T))\n    for(i in test.statistics[subset]){\n      data[,i] <- data[,i]/maxstats[i]\n    }\n    stat.obs <<- test.statistics.obs[test.statistics[subset]]/maxstats\n  } else{\n    stat.obs <<- test.statistics.obs[test.statistics[subset]]\n  }\n  stat.sim <<- data[,test.statistics[subset],drop=FALSE]\n}\n\n\n## plot priors for all parameters (to check for uniformity) \n\nvisualize.priors <- function(){\n  par(mfrow=c(2, 3),ask=TRUE)\n  \n  for(i in 1:length(estimable.params)){\n    hist(all[,estimable.params[i]], breaks = 50,main=estimable.params[i])\n  }\n}\n\n\n## note: this function only works with rejection method...\nvisualize.posteriors <- function(){\n  par(mfrow=c(2, 3),ask=TRUE)\n  posterior_ndx <- as.numeric(rownames(as.matrix(rej$ss))) \n  all_post <- all[posterior_ndx,]\n  for(i in 1:length(estimable.params)){\n    hist(all_post[,estimable.params[i]], breaks = 50,main=estimable.params[i],freq=FALSE)\n    lines(density(all_post[,estimable.params[i]]),col=\"blue\",lwd=2)\n  }\n}\n\n\n\nvisualize <- function(target=target){\n  \n  par(ask=TRUE)\n  \n  #if(toPlot==\"ABUNDANCE\"){\n    matches <- as.numeric(unlist(regmatches(target, gregexpr(\"[[:digit:]]+\", target))))\n    nichebreadth <- matches[1]\n    sample <- matches[2]\n    \n    \n    filename <- sprintf(\"NicheBreadth%i_TotAbund_output_alldata.csv\",nichebreadth)\n    \n    setwd(SIM_RESULTS_DIR)\n    \n    #getwd()\n    #list.files()\n    \n    \n    abund <- read.csv(filename, header = T,stringsAsFactors = FALSE)    # if(!\"abund\"%in%ls(name=.GlobalEnv))  \n  \n    rownames(abund) <- abund[,1]\n  \n    #head(abund[,2000:2010])\n    \n    time<- rev(seq(from =0, to =80000, by = 25))\n    \n    timeall <- time[2361:2928]\n    timebreak1 <- time[2708:2928]\n    timebreak2 <- time[2361:2707]\n    \n    ## Provide row id of simulation of interest\n    abundall <- as.numeric(abund[target,2362:2929])     \n    abundbreak1 <- as.numeric(abund[target,2709:2929])\n    abundbreak2 <- as.numeric(abund[target,2362:2708])\n    \n    #graphics.off()\n    temp = sprintf(\"abundfig_%s.pdf\",filename)\n    pdf(file = temp,width = 5,height=5)\n    plot(timeall, abundall, pch = 16, cex = 0.2, col = \"blue\", main = \"Change in Ne\", \n         ylim=c(0,min(2000000,max(abundall+10000))),xlab = \"Time (year)\", ylab = \"Abundance\")   #  ,\n    \n    break1coef <- round(lm(abundbreak1~timebreak1)$coefficients[2],2) \n    break2coef <- round(lm(abundbreak2~timebreak2)$coefficients[2],2)\n    \n    abline(lm(abundbreak1~timebreak1))\n    abline(lm(abundbreak2~timebreak2))\n    \n    \n    ## Extinction presumed to have occurred by 3000 years ago; and only interested in records after 21,000 years ago\n    time2 <- (eff_popsize[61:420,1])\n    abund2 <-(eff_popsize[61:420,2])\n    data2 <- cbind(time2, abund2)\n    \n    points(time2, abund2, cex = 0.1, col = \"red\",pch=20)\n  #}\n    dev.off()\n  \n  \n  #if(toPlot==\"EXTINCTION_DATE\"){\n    matches <- as.numeric(unlist(regmatches(target, gregexpr(\"[[:digit:]]+\", target))))\n    nichebreadth <- matches[1]\n    sample <- matches[2]\n    \n    colors <- c(\"red\",\"orange\",\"yellow\",\"green\",\"blue\",\"purple\",\"violet\",\"black\")\n    cutoffs <- c(100000,70000,40000,20000,15000,10000,5000,4000,0)\n    \n    #colname <- sprintf(\"NicheBreadth%i_LHS_Sample%i.mp\",nichebreadth,sample)\n    \n    filename <- sprintf(\"NicheBreadth%i_FinalYear_output_alldata_formatted.csv\",nichebreadth)\n    \n    setwd(SIM_RESULTS_DIR)\n    \n    extinct <- read.csv(filename, header = T,stringsAsFactors = FALSE)    # if(!\"abund\"%in%ls(name=.GlobalEnv))  \n    \n    #rownames(extinct) <-  \n    \n    xcords <- extinct[,1]\n    ycords <- extinct[,2]\n    head(extinct[,1:10])\n    \n    if(any(xcords<0)) xcords[which(xcords<0)] <- 180+(180 - abs(xcords[which(xcords<0)]))\n    \n    thissample <- extinct[,target]\n    \n    time<- rev(seq(from =0, to =80000, by = 25))\n    \n    ext.time <- time[thissample]\n    \n    ext.time.cols <- 9-as.numeric(cut(ext.time,cutoffs))\n    \n    #exttime_scaled <- ext.time/80000\n    \n    cols <- colors[ext.time.cols] #colorRamp(c(\"blue\",\"red\"), bias = 1, space = c(\"rgb\"),\n             #         interpolate = c(\"linear\"), alpha = FALSE)(exttime_scaled)\n    \n    #graphics.off()\n    #map <- get_map(location = 'Asia', zoom = 3)\n    newmap <- getMap(resolution = \"low\")\n    #ggmap(map)\n    \n    \n    \n    par(mfrow=c(2,1))\n    plot(newmap,xlim = c(20, 200),\n         ylim = c(30, 80),\n         asp = 1,\n         main=\"simulated\"\n    )\n    points(xcords,ycords,pch=20,cex=0.01,col=cols)\n    \n    ### location of final extinction\n    ndx <- which(ext.time==min(ext.time,na.rm=T))\n    xcords_lastext <- extinct[ndx,1]\n    \n    if(any(xcords_lastext<0)) xcords_lastext[which(xcords_lastext<0)] <- 180+(180 - abs(xcords_lastext[which(xcords_lastext<0)]))\n    \n    ycoord_lastext <- extinct[ndx,2]\n    \n    # plot(newmap,xlim = c(50, 219),\n    #      ylim = c(30, 71),\n    #      asp = 1,\n    #      main=\"real\"\n    # )\n    points(xcords_lastext,ycoord_lastext,pch=\"X\",cex=2,col=\"black\")\n    \n    \n    plot(newmap,xlim = c(20, 200),\n         ylim = c(30, 80),\n         asp = 1,\n         main=\"real\"\n    )\n    \n    ext.time.cols <- 9-as.numeric(cut(extinct_date$Ext,cutoffs))\n    \n    #exttime_scaled <- ext.time/80000\n    \n    cols <- colors[ext.time.cols]\n    #cols <- colorRamp(c(\"blue\",\"red\"), bias = 1, space = c(\"rgb\"),\n     #                 interpolate = c(\"linear\"), alpha = FALSE)(extinct_date$Ext/80000)\n\n    points(extinct_date$Mammoth_estimates.CI.Barnoski._Long,extinct_date$Mammoth_estimates.CI.Barnoski._Lat,pch=20,cex=0.01,col=cols)    \n    \n\n    points(last.locations[5,],pch=\"X\",cex=2,col=\"yellow\")\n    \n    \n    \n    \n  #}\n  #graphics.off()\n  \n}\n\n\n###############\n# MAKE MOVIE   (NOTE: need ffmpeg and imagemagick installed)\n###############\n\n# for testing\ntarget = \"NicheBreadth70_LHS_Sample1.mp\"\n\nMakeMovie <- function(target){\n  \n  setwd(SIM_RESULTS_DIR)\n  \n  ############\n  # SET UP STORAGE STRUCTURES FOR KEY RESULTS METRICS\n  ############\n  \n  ###########################\n  # GET RESULTS\n  ###########################\n  \n  result = tryCatch({   ## try to catch errors! \n    ### step 1: set up the MP file connection \n    \n    MPcon <- file(target, 'r')\n    \n    # while loop: find the string \"Pop. 1\" [indicates the beginning of the population results]\n    stringToFind <- \"Pop. 1\"\n    basendx <- 0\n    CHUNKSIZE <- 1000\t\t\n    \n    \n    while (length(input <- readLines(MPcon, n=CHUNKSIZE)) > 0){    # read in chunks until population results are found\n      \n      temp <- grep(stringToFind,input)\n      if(length(temp)>0){ \n        ndx <- basendx + temp\n        pushBack(input[(temp):CHUNKSIZE],MPcon)       # reset the file to where Pop. 1 began\n        break\n      } \n      basendx <- basendx + CHUNKSIZE\n      \n    }   # end while loop\n    \n    PopMat <- array(0,dim=c(NPOPS,TIMESTEPS))\n    \n    # read in the population abundances over time\n    for(pop in 1:NPOPS){\n      stringToFind <- sprintf(\"Pop. %s\",pop)\n      temp <- readLines(MPcon,1)\n      if(temp!=stringToFind){\n        print(paste(\"ERROR!\",\"Population #\",pop))\n        break\n      }\n      input <- readLines(MPcon, n=TIMESTEPS)\n      Nvec <- sapply( strsplit(input, \" \"), function(t) as.numeric(t[1]))\n      PopMat[pop,] <- Nvec\n      #eval(parse(text=sprintf(\"SimInfo$PopAbund[%s,] <- Nvec\",pop)))\t   # RESULT: POP ABUNDANCE             \n    }   # end loop through pops \n    \n    if(isOpen(MPcon)){ \n      close.connection(MPcon)\n      rm(\"MPcon\")\n    }\n    \n    if(exists(\"MPcon\")){\n      rm(\"MPcon\")\n    }\n  }, warning = function(w){\n    as.character(w)\n  }, error = function(e){\n    setwd(thisFolder)\n    filename <- sprintf(\"%s.RData\",name)\n    eval(parse(text=sprintf(\"%s <- e\",name)))\n    eval(parse(text=sprintf(\"save(%s,file=filename)\",name)))   # save to disk\n    eval(parse(text=sprintf(\"rm(%s)\",name)))   # remove from memory\n    as.character(e)\n  }, finally = {\n    SimInfo$PopAbund <- NULL   ## remove from memory\n    if(exists(\"MPcon\")){\n      close.connection(MPcon)\n      rm(\"MPcon\")\n    }\n  })   # end tryCatch\n\n  \n  thisMOVIE_DIR <- sprintf(\"%s\\\\%s\",getwd(),gsub(\"\\\\.mp\",\"\",target))\n  if(is.na(file.info(thisMOVIE_DIR)[1,\"isdir\"])) dir.create(thisMOVIE_DIR)\n  \n  setwd(thisMOVIE_DIR)\n  \n  #### remove figures that are not part of this simulation?\n  \n  fileyrs <- as.numeric(unlist(regmatches(list.files(), gregexpr(\"[[:digit:]]+\", list.files()))))\n  notthissim <- fileyrs > TIMESTEPS\n  toremove <- list.files()[notthissim]\n  \n  if(any(notthissim)) file.remove(toremove)\n  \n  ################\n  # MAKE PLOTS\n  \n  width = 800\n  height= 300\n\n  t = 1\n  counter=1\n  for(t in seq(1,TIMESTEPS,4)){\n    file = sprintf(\"AbundanceMap_year%04d.tif\",counter)\n    tiff(file, width=width,height=height)\n    \n    #colors <- c(\"red\",\"orange\",\"yellow\",\"green\",\"blue\",\"purple\",\"violet\",\"black\",\"black\")\n    colors <- viridis::plasma(9)\n    \n    \n    \n    cutoffs <- c(100000,10000,5000,2500,1000,500,100,10,0,-1)\n    \n    #rownames(extinct) <-  \n    \n    xcords <- GridCellAttributes$x.cord\n    ycords <- GridCellAttributes$y.cord\n    head(extinct[,1:10])\n    \n    if(any(xcords<0)) xcords[which(xcords<0)] <- 180+(180 - abs(xcords[which(xcords<0)]))\n    \n    time<- rev(seq(from =0, to =80000, by = 25))\n    \n    abundnow <- PopMat[,t]\n    \n    abund.cols <- 10-as.numeric(cut(abundnow,cutoffs))\n    \n    cols <- colors[abund.cols] #colorRamp(c(\"blue\",\"red\"), bias = 1, space = c(\"rgb\"),\n    #         interpolate = c(\"linear\"), alpha = FALSE)(exttime_scaled)\n    \n    newmap <- getMap(resolution = \"low\")\n    \n    plot(newmap,xlim = c(20, 200),\n         ylim = c(30, 80),\n         asp = 1,\n         main=sprintf(\"%s years bp\",time[t])\n    )\n    nonzero <- abundnow>0\n    points(xcords[nonzero],ycords[nonzero],pch=20,cex=0.01,col=cols[nonzero])\n  \n   \n    dev.off() \n    counter=counter+1\n  }\n  \n  ## NOTE: need command line like this: ffmpeg -f image2 -framerate 2 -i AbundanceMap_year%03d.tif -s 500x500 test.avi -y\n  \n  # MAKING THE REAL MOVIE HERE! USE IMAGE MAGICK AND FFMPEG SOFTWARE  (https://blogazonia.wordpress.com/2016/01/19/making-a-movie-with-r/)\n  \n  \n  # create the movie\n  cmd_abundmov <- paste0(\"ffmpeg -f image2 -framerate 2 -i AbundanceMap_year%04d.tif -s 800x300 \", \n                         sprintf(\"%s\\\\AbundanceMovie.avi\",thisMOVIE_DIR),\" -y\")\n  \n  \n  #sink(tempfile())\n  system(cmd_abundmov,ignore.stdout = T,ignore.stderr = T)\n  \n  #sink()\n}  ### end function \"MakeMovie\"\n\n\n\n\n####################\n# LOAD SIMULATIONs\n#\n# specify directory that contains all simulation results and parameter values, prepared for ABC\n###################\n\nKEVIN_LAB <- TRUE # FALSE\nDAMIEN <- FALSE # TRUE \n\nif(KEVIN_LAB) BASE_RESULTS_DIR <- \"E:\\\\MammothResults\"\nif(DAMIEN) BASE_RESULTS_DIR <- \"F:\\\\MammothResultsV2\"\n\nSIM_RESULTS_DIR <- sprintf(\"%s\\\\ABCAnalysisInputs\\\\\",BASE_RESULTS_DIR)\nTARGETS_DIR <- sprintf(\"%s\\\\ABCAnalysisTargets\",BASE_RESULTS_DIR)\nGENETIC_TARGETS_DIR <- sprintf(\"%s\\\\Genetic\",TARGETS_DIR)\nFOSSIL_TARGETS_DIR <- sprintf(\"%s\\\\Fossil\",TARGETS_DIR)   \nINPUTS_DIR <- sprintf(\"%s\\\\ABCAnalysis\",BASE_RESULTS_DIR)\n\nsetwd(INPUTS_DIR)\n\n\ndat1 <- read.csv(\"NicheBreadth40_ABC_data.csv\", header = T,stringsAsFactors = FALSE)\ndat2 <- read.csv(\"NicheBreadth50_ABC_data.csv\", header = T,stringsAsFactors = FALSE)\ndat3 <- read.csv(\"NicheBreadth60_ABC_data.csv\", header = T,stringsAsFactors = FALSE)\ndat4 <- read.csv(\"NicheBreadth70_ABC_data.csv\", header = T,stringsAsFactors = FALSE)\ndat5 <- read.csv(\"NicheBreadth80_ABC_data.csv\", header = T,stringsAsFactors = FALSE)\ndat6 <- read.csv(\"NicheBreadth90_ABC_data.csv\", header = T,stringsAsFactors = FALSE)\n\nall <- rbind(dat1,dat2,dat3,dat4,dat5,dat6)\n\nnrow(all)   # 89999 simulations\n\nhead(all)   # check the combined dataframe \n\n## Generate a version with no NA values (some analyses do not allow NA values)\n\nall_nona <- na.omit(all)   ### remove all na observations\n\n\n### Generate a version with no persistent simulations (remove all sims that never went extinct or extinct in first time step)\n\nkeep <- !is.na(all$extinction_yr)\nlength(which(keep))   # 20126 simulations remaining\nnrow(all)-length(which(keep))   # 20087 sims did not undergo global extinction within the simulation period\n\nall_extinct <- all[keep,]     # remove the ca. 20k simulations that were not plausible.\n\n\n# explore any remaining NAs in the data frame, after removal of the non-extinctions...\nremaining.nas <- all_extinct[which(is.na(all_extinct),arr.ind=TRUE)[,1],]\n\nnrow(remaining.nas)     # 306 simulations with NAs for dist_centroid and dist_fossil\nremaining.nas$dist_fossil\nremaining.nas$dist_centroid\ntail(remaining.nas)\n\n\n\n####################\n# LOAD REAL-WORLD DATA \n\n# targets for matching with simulated data- for ABC\n####################\n\nsetwd(GENETIC_TARGETS_DIR)\n  \n    # load data on effective population size from genetic data\neff_popsize <- read.csv(\"Mammoth_Ne.csv\", header = T)\n\n\nsetwd(FOSSIL_TARGETS_DIR)\n\n  # load data on extinction date from the fossil record -- INTERPOLATED  STATISTICAL DATA\nextinct_date <- read.csv(\"Barnoski_matched_estimates_fossils.csv\", header = T)\n\nextinct_date$Mammoth_estimates.CI.Barnoski._Long[which(extinct_date$Mammoth_estimates.CI.Barnoski._Long<0)]<- 180+(180 - abs(extinct_date$Mammoth_estimates.CI.Barnoski._Long[which(extinct_date$Mammoth_estimates.CI.Barnoski._Long<0)]))\n\n\n  # Generate a version that has extinction date only for sites with fossil records (not interpolated)\nextinct_date_fossilsites <- extinct_date[extinct_date$Fossil.Data==1,]\n\n    # final locations of extinction, observed\nlast.locations <- extinct_date[which(extinct_date$Ext==min(extinct_date$Ext)),c(1,2)]\n\nnames(last.locations) <- c(\"X\",\"Y\")\n\nhist(extinct_date$Ext)\n\n####################\n# OBSERVED STATISTICS FOR MATCHING WITH SIMULATION RESULTS\n###################\n\ntest.statistics.obs <- c(64.0765, 11.4448, 0, 0, 3993, 0, 0)\n\nnames(test.statistics.obs) <- c(\"abundcoef1\",\"abundcoef2\",\"extinctpattern_all\",\"extinctpattern_fossilsites\",\"extinction_yr\",\"dist_centroid\",\"dist_fossil\")\n\n\ntest.statistics.obs\n\n####################\n# SET UP VARIABLEs FOR ABC\n\n# All variable (estimable) parameters and all summary statistics for matching with data\n###################\n\n      #### What is the multivariate parameter space we are attempting to estimate?\n\nestimable.params <- c(\n  \"RMAX\",      \n  \"SD\",        # env. stochasticity\n  \"ALLEE\",     \n  \"DENSITY\",   # max density of mammoths\n  \"DISP1\",     # determines rate of staying/leaving a given cell\n  \"DISP2\",     # max dispersal distance\n  \"HARV\",      # maximal harvest rate\n  \"HARVZ\",     # regulates shape of functional response\n  \"HUMAN\",\n  \"summ_precip_median\",\n  \"summ_precip_range\",\n  \"jan_temp_median\",\n  \"jan_temp_range\",\n  \"jul_temp_median\",\n  \"jul_temp_range\"\n)\n\nestimable.params\n\n   #### Which test statistics should we use? \n\ntest.statistics <- c(\n  \"abundcoef1\",    # slope of abundance decline, most recent\n  \"abundcoef2\",     # slope of abuncance decline, older\n  \"extinctpattern_all\",\n  \"extinctpattern_fossilsites\",\n  \"extinction_yr\",\n  \"dist_centroid\",\n  \"dist_fossil\"\n)\n\ntest.statistics\n\n\n### NOTES: which summary statistics to use??\n\n## use just coefficient 1 (abundance by itself) \n## timing of extinction is critical\n## play around with both distance measures but don't use both at the same time because meaningless...\n## pattern by itself\n## pattern with extinction?   bring NAs in?\n\n\n#####################\n# PERFORM BASIC VISUALIZATIONS AND DATA CHECKS\n#####################\n \n### VISUALIZE TEST STATISTICS  (with targets overlaid)\n\npar(mfrow=c(2,3))\nfor(i in test.statistics){\n  hist(all[,i],main=i)\n  abline(v=test.statistics.obs[i],col=\"green\",lwd=2)\n}\n\n\n### VISUALIZE PRIORS (check for uniformity- don't need to run this every time)\n\nvisualize.priors()\n\n####################\n# SELECT SUMMARY STATISTICS\n####################\n\n?selectsumm\n\ntest <- prepareForABC(all_nona,subset=c(1:7),standardized = TRUE)\n\nselectstats <- abctools::selectsumm(test.statistics.obs, par.sim, \n                                     stat.sim, ssmethod =mincrit,  # AS.select,    #  \n                                     final.dens = FALSE)\n\n# which summary statistics were selected by the \"selectsumm\" algorithm?   # # given by selectstats$best\ncolnames(selectstats$best) <- test.statistics\n\nselectstats$best\n\n#### make final selections...\n\ntest.statistics\n\n\n   ## here we subset the targets to the best set identified above. This time extinction year is best\nprepareForABC(all,subset=c(4,5,6),standardized=T)\n\n\nhead(par.sim)\nhead(stat.sim)   # make sure the right summary statistics are being used\nstat.obs\n\n\n#####################\n# PERFORM PRELIMINARY ABC, VISUALIZATIONS AND DATA CHECKS\n#####################\n\n### Using the rejection Method to find the best fits\n\n\n#####\n# ABC analysis\n#####\nrej <- abc(target=stat.obs, param=par.sim, sumstat=stat.sim, tol=.005, \n           method =\"neuralnet\", numnet=20, sizenet=10,trace=T,maxit=1000,lambda = c(0.0001,0.001,0.01))   # rejection  # loclinear\n\n\n\nstr(rej)\n\n# NOTE: neural net method seems to perform the best...\n\n   # explore the abc object...\nhead(rej$adj.values)\nhead(rej$unadj.values)\nrej$weights\n\nrej$dist    # this is the euclidean distance between simulation results and observations \n\n######     \n # summary and diagnostic plots\n######\n\nplot(rej,param=par.sim)     # visualize all priors and posteriors\n\nrej$ss  # all simulation runs that were kept for the posterior\n\n    ### credible intervals\nsummary(rej)\n\n\n\n    ### cross validation (can take a while with neural net)\n\n### the cross validation does NOT use the observed targets. Instead, it takes simulated data, one at a time, as\n#    the \"observed\" targets, and uses the remaining simulations to perform an ABC analysis with the new\n#    \"observed\" summary statistics. \n#    this way, we can look at the overall quality of the ABC (ability to estimate selected parameters) \n#    if the algorithm is good (summary stats are informative) then the parameters estimated by ABC should\n#    match the parameters of the simulation that in fact generated the psuedo-observed data... \n\ncv.res.reg <- cv4abc(data.frame(Harv=par.sim$HARV,Harvz=par.sim$HARVZ,Jantemp=par.sim$jan_temp_median), stat.sim,\n                     nval=20, tols=c(0.01,0.005),   # method=\"rejection\")\n                     method =\"neuralnet\", numnet=20, sizenet=10,trace=F,maxit=1000,lambda = c(0.0001,0.001,0.01)) \nplot(cv.res.reg)\n\nsummary(cv.res.reg)   # good summary of model performance\n\n\n\n#############################\n# VISUALIZE ABUNDANCE TRAJECTORIES AND EXTINCTION TIMES\n#############################\n\n### find the best simulations to explore further\n\nif(length(test.statistics)>1){\n  ndx <- as.numeric(rownames(rej$ss))\n  if(is.null(ndx)){\n    intersect(rej$adj.values,par.sim)\n  }\n  #ranking <- order(apply(as.matrix(rej$ss,min),1,sum))\n  ranking <- order(as.vector(rej$weights))\n  ndx <- ndx[ranking]\n  ndx_min <- as.numeric(names(which.min(apply(as.matrix(rej$ss,min),1,sum))))\n}else{\n  ndx <- as.numeric(names(rej$ss))\n  ranking <- order(as.vector(rej$ss))\n  ndx <- ndx[ranking]\n  #ndx_min <- as.numeric(names(which(rej$ss == min(rej$ss), arr.ind = TRUE)))[1]\n  #mins <- apply(as.matrix(rej$ss,min),2,min)\n}\n\n\n\nindex <- ndx[1]   # one of a set of all models used in the posterior\nindex <- ndx_min   # the best fit model\n\ntarget <- all[index,]$model\n\n#toPlot <-  \"EXTINCTION_DATE\"  # \"ABUNDANCE\"     # \n\n\n\n\n### NOTE on color scheme: red is >70k time to extinction. \n                      # orange is from 40 to 70k\n                      # yellow is from 20 to 40k\n                      # green is from 15 to 20k\n                      # blue is from 10 to 15k\n                      # purple is from 5 to 10k\n                      # indigo is from 4-5k\n                      # black is from 3-4k\n\ngraphics.off()\nvisualize(target)\n\n\n\n#####################\n# VISUALIZE POSTERIORS\n#####################\n\n\n    # only works for elimination method\n # visualize.posteriors()\n\n\n\n#####################\n# from ABC package\n\n## Note: cv4postpr is only for model selection...\n          # also \"postpr\" is only for model selection\n          # use \"cv4abc\" function for our purposes\n\n\n## Note: the neural network method seems to be the best- local linear does not work at all!\n\n\n##### TESTS\n\n# does removing nas do anything to the results? it shouldn't!\n# does standardizing the targets have an effect?\n\n\n\n\n\n######\n\n\nfunction (target, param, sumstat, tol, method, hcorr = TRUE, \n          transf = \"none\", logit.bounds = c(0, 0), subset = NULL, kernel = \"epanechnikov\", \n          numnet = 10, sizenet = 5, lambda = c(1e-04, 0.001, 0.01), \n          trace = FALSE, maxit = 500, ...) \n{\n  call <- match.call()\n  if (missing(target)) \n    stop(\"'target' is missing\")\n  if (missing(param)) \n    stop(\"'param' is missing\")\n  if (missing(sumstat)) \n    stop(\"'sumstat' is missing\")\n  if (!is.matrix(param) && !is.data.frame(param) && !is.vector(param)) \n    stop(\"'param' has to be a matrix, data.frame or vector.\")\n  if (!is.matrix(sumstat) && !is.data.frame(sumstat) && !is.vector(sumstat)) \n    stop(\"'sumstat' has to be a matrix, data.frame or vector.\")\n  if (missing(tol)) \n    stop(\"'tol' is missing\")\n  if (missing(method)) \n    stop(\"'method' is missing with no default\")\n  if (!any(method == c(\"rejection\", \"loclinear\", \"neuralnet\", \n                       \"ridge\"))) {\n    stop(\"Method must be rejection, loclinear, or neuralnet or ridge\")\n  }\n  if (method == \"rejection\") \n    rejmethod <- TRUE\n  else rejmethod <- FALSE\n  if (!any(kernel == c(\"gaussian\", \"epanechnikov\", \"rectangular\", \n                       \"triangular\", \"biweight\", \"cosine\"))) {\n    kernel <- \"epanechnikov\"\n    warning(\"Kernel is incorrectly defined. Setting to default (Epanechnikov)\")\n  }\n  if (is.data.frame(param)) \n    param <- as.matrix(param)\n  if (is.data.frame(sumstat)) \n    sumstat <- as.matrix(sumstat)\n  if (is.list(target)) \n    target <- unlist(target)\n  if (is.vector(sumstat)) \n    sumstat <- matrix(sumstat, ncol = 1)\n  if (length(target) != dim(sumstat)[2]) \n    stop(\"Number of summary statistics in 'target' has to be the same as in 'sumstat'.\")\n  nss <- length(sumstat[1, ])\n  cond1 <- !any(as.logical(apply(sumstat, 2, function(x) length(unique(x)) - \n                                   1)))\n  if (cond1) \n    stop(\"Zero variance in the summary statistics.\")\n  ltransf <- length(transf)\n  if (is.vector(param)) {\n    numparam <- 1\n    param <- matrix(param, ncol = 1)\n  }\n  else numparam <- dim(param)[2]\n  for (i in 1:ltransf) {\n    if (sum(transf[i] == c(\"none\", \"log\", \"logit\")) == 0) {\n      stop(\"Transformations must be none, log, or logit.\")\n    }\n    if (transf[i] == \"logit\") {\n      if (logit.bounds[i, 1] >= logit.bounds[i, 2]) {\n        stop(\"Logit bounds are incorrect.\")\n      }\n    }\n  }\n  if (rejmethod) {\n    if (!all(transf == \"none\")) {\n      warning(\"No transformation is applied when the simple rejection is used.\", \n              call. = F)\n    }\n    transf[1:numparam] <- \"none\"\n  }\n  else {\n    if (numparam != ltransf) {\n      if (length(transf) == 1) {\n        transf <- rep(transf[1], numparam)\n        warning(\"All parameters are \\\"\", transf[1], \"\\\" transformed.\", \n                sep = \"\", call. = F)\n      }\n      else stop(\"Number of parameters is not the same as number of transformations.\", \n                sep = \"\", call. = F)\n    }\n  }\n  gwt <- rep(TRUE, length(sumstat[, 1]))\n  gwt[attributes(na.omit(sumstat))$na.action] <- FALSE\n  if (missing(subset)) \n    subset <- rep(TRUE, length(sumstat[, 1]))\n  gwt <- as.logical(gwt * subset)\n  if (!length(colnames(param))) {\n    warning(\"No parameter names are given, using P1, P2, ...\")\n    paramnames <- paste(\"P\", 1:numparam, sep = \"\")\n  }\n  else paramnames <- colnames(param)\n  if (!length(colnames(sumstat))) {\n    warning(\"No summary statistics names are given, using S1, S2, ...\")\n    statnames <- paste(\"S\", 1:nss, sep = \"\")\n  }\n  else statnames <- colnames(sumstat)\n  scaled.sumstat <- sumstat\n  for (j in 1:nss) {\n    scaled.sumstat[, j] <- normalise(sumstat[, j], sumstat[, \n                                                           j][gwt])\n  }\n  for (j in 1:nss) {\n    target[j] <- normalise(target[j], sumstat[, j][gwt])\n  }\n  sum1 <- 0\n  for (j in 1:nss) {\n    sum1 <- sum1 + (scaled.sumstat[, j] - target[j])^2\n  }\n  dist <- sqrt(sum1)\n  dist[!gwt] <- floor(max(dist[gwt]) + 10)\n  nacc <- ceiling(length(dist) * tol)\n  ds <- sort(dist)[nacc]\n  wt1 <- (dist <= ds)\n  aux <- cumsum(wt1)\n  wt1 <- wt1 & (aux <= nacc)\n  if (kernel == \"gaussian\") {\n    wt1 <- rep(TRUE, length(dist))\n  }\n  for (i in 1:numparam) {\n    if (transf[i] == \"log\") {\n      if (min(param[, i]) <= 0) {\n        cat(\"log transform: values out of bounds - correcting...\")\n        x.tmp <- ifelse(param[, i] <= 0, max(param[, \n                                                   i]), param[, i])\n        x.tmp.min <- min(x.tmp)\n        param[, i] <- ifelse(param[, i] <= 0, x.tmp.min, \n                             param[, i])\n      }\n      param[, i] <- log(param[, i])\n    }\n    else if (transf[i] == \"logit\") {\n      if (min(param[, i]) <= logit.bounds[i, 1]) {\n        x.tmp <- ifelse(param[, i] <= logit.bounds[i, \n                                                   1], max(param[, i]), param[, i])\n        x.tmp.min <- min(x.tmp)\n        param[, i] <- ifelse(param[, i] <= logit.bounds[i, \n                                                        1], x.tmp.min, param[, i])\n      }\n      if (max(param[, i]) >= logit.bounds[i, 2]) {\n        x.tmp <- ifelse(param[, i] >= logit.bounds[i, \n                                                   2], min(param[, i]), param[, i])\n        x.tmp.max <- max(x.tmp)\n        param[, i] <- ifelse(param[, i] >= logit.bounds[i, \n                                                        2], x.tmp.max, param[, i])\n      }\n      param[, i] <- (param[, i] - logit.bounds[i, 1])/(logit.bounds[i, \n                                                                    2] - logit.bounds[i, 1])\n      param[, i] <- log(param[, i]/(1 - param[, i]))\n    }\n  }\n  ss <- sumstat[wt1, ]\n  unadj.values <- param[wt1, ]\n  statvar <- as.logical(apply(cbind(sumstat[wt1, ]), 2, function(x) length(unique(x)) - \n                                1))\n  cond2 <- !any(statvar)\n  if (cond2 && !rejmethod) \n    stop(\"Zero variance in the summary statistics in the selected region. Try: checking summary statistics, choosing larger tolerance, or rejection method.\")\n  if (rejmethod) {\n    if (cond2) \n      warning(\"Zero variance in the summary statistics in the selected region. Check summary statistics, consider larger tolerance.\")\n    weights <- rep(1, length = sum(wt1))\n    adj.values <- NULL\n    residuals <- NULL\n    lambda <- NULL\n  }\n  else {\n    if (cond2) \n      cat(\"Warning messages:\\nStatistic(s)\", statnames[!statvar], \n          \"has/have zero variance in the selected region.\\nConsider using larger tolerance or the rejection method or discard this/these statistics, which might solve the collinearity problem in 'lsfit'.\\n\", \n          sep = \", \")\n    if (kernel == \"epanechnikov\") \n      weights <- 1 - (dist[wt1]/ds)^2\n    if (kernel == \"rectangular\") \n      weights <- dist[wt1]/ds\n    if (kernel == \"gaussian\") \n      weights <- 1/sqrt(2 * pi) * exp(-0.5 * (dist/(ds/2))^2)\n    if (kernel == \"triangular\") \n      weights <- 1 - abs(dist[wt1]/ds)\n    if (kernel == \"biweight\") \n      weights <- (1 - (dist[wt1]/ds)^2)^2\n    if (kernel == \"cosine\") \n      weights <- cos(pi/2 * dist[wt1]/ds)\n    if (method == \"loclinear\") {\n      fit1 <- lsfit(scaled.sumstat[wt1, ], param[wt1, ], \n                    wt = weights)\n      pred <- t(structure(cbind(fit1$coefficients)[fit1$qr$pivot, \n                                                   ], names = names(fit1$coefficients))) %*% c(1, \n                                                                                               target)\n      pred <- matrix(pred, ncol = numparam, nrow = sum(wt1), \n                     byrow = TRUE)\n      residuals <- param[wt1, ] - t(t(structure(cbind(fit1$coefficients)[fit1$qr$pivot, \n                                                                         ], names = names(fit1$coefficients))) %*% t(cbind(1, \n                                                                                                                           scaled.sumstat[wt1, ])))\n      residuals <- cbind(residuals)\n      the_m <- apply(residuals, FUN = mean, 2)\n      residuals <- sapply(1:numparam, FUN = function(x) {\n        residuals[, x] - the_m[x]\n      })\n      pred <- sapply(1:numparam, FUN = function(x) {\n        pred[, x] + the_m[x]\n      })\n      sigma2 <- apply(as.matrix(residuals), FUN = function(x) {\n        sum((x)^2 * weights)/sum(weights)\n      }, MARGIN = 2)\n      aic <- sum(wt1) * sum(log(sigma2)) + 2 * (nss + 1) * \n        numparam\n      bic <- sum(wt1) * sum(log(sigma2)) + log(sum(wt1)) * \n        (nss + 1) * numparam\n      if (hcorr == TRUE) {\n        fit2 <- lsfit(scaled.sumstat[wt1, ], log(residuals^2), \n                      wt = weights)\n        auxaux <- t(structure(cbind(fit2$coefficients)[fit2$qr$pivot, \n                                                       ], names = names(fit2$coefficients))) %*% c(1, \n                                                                                                   target)\n        pred.sd <- sqrt(exp(auxaux))\n        pred.sd <- matrix(pred.sd, nrow = sum(wt1), ncol = numparam, \n                          byrow = T)\n        pred.si <- t(t(structure(cbind(fit2$coefficients)[fit2$qr$pivot, \n                                                          ], names = names(fit2$coefficients))) %*% t(cbind(1, \n                                                                                                            scaled.sumstat[wt1, ])))\n        pred.si <- sqrt(exp(pred.si))\n        adj.values <- pred + (pred.sd * residuals)/pred.si\n        residuals <- (pred.sd * residuals)/pred.si\n      }\n      else {\n        adj.values <- pred + residuals\n      }\n      colnames(adj.values) <- colnames(unadj.values)\n      lambda <- NULL\n    }\n    if (method == \"neuralnet\") {\n      linout <- TRUE\n      param.mad <- c()\n      for (i in 1:numparam) {\n        param.mad[i] <- mad(param[, i][gwt])\n        param[, i] <- normalise(param[, i], param[, i][gwt])\n      }\n      lambda <- sample(lambda, numnet, replace = T)\n      fv <- array(dim = c(sum(wt1), numparam, numnet))\n      pred <- matrix(nrow = numparam, ncol = numnet)\n      for (i in 1:numnet) {\n        fit1 <- nnet(scaled.sumstat[wt1, ], param[wt1, \n                                                  ], weights = weights, decay = lambda[i], size = sizenet, \n                     trace = trace, linout = linout, maxit = maxit, \n                     ...)\n        cat(i)\n        fv[, , i] <- fit1$fitted.values\n        pred[, i] <- predict(fit1, data.frame(rbind(target)))\n      }\n      cat(\"\\n\")\n      pred.med <- apply(pred, 1, median)\n      pred.med <- matrix(pred.med, nrow = sum(wt1), ncol = numparam, \n                         byrow = T)\n      fitted.values <- apply(fv, c(1, 2), median)\n      residuals <- param[wt1, ] - fitted.values\n      if (hcorr == TRUE) {\n        pred2 <- matrix(nrow = numparam, ncol = numnet)\n        fv2 <- array(dim = c(sum(wt1), numparam, numnet))\n        for (i in 1:numnet) {\n          fit2 <- nnet(scaled.sumstat[wt1, ], log(residuals^2), \n                       weights = weights, decay = lambda[i], size = sizenet, \n                       trace = trace, linout = linout, ...)\n          cat(i)\n          fv2[, , i] <- fit2$fitted.values\n          pred2[, i] <- predict(fit2, data.frame(rbind(target)))\n        }\n        cat(\"\\n\")\n        pred.sd <- sqrt(exp(apply(pred2, 1, median)))\n        pred.sd <- matrix(pred.sd, nrow = sum(wt1), ncol = numparam, \n                          byrow = T)\n        fv.sd <- sqrt(exp(apply(fv2, c(1, 2), median)))\n        adj.values <- pred.med + (pred.sd * residuals)/fv.sd\n        residuals <- (pred.sd * residuals)/fv.sd\n      }\n      else {\n        adj.values <- pred.med + residuals\n      }\n      colnames(adj.values) <- colnames(unadj.values)\n      for (i in 1:numparam) {\n        adj.values[, i] <- adj.values[, i] * param.mad[i]\n      }\n    }\n    if (method == \"ridge\") {\n      param.mad <- c()\n      for (i in 1:numparam) {\n        param.mad[i] <- mad(param[, i][gwt])\n        param[, i] <- normalise(param[, i], param[, i][gwt])\n      }\n      numnet <- length(lambda)\n      fv <- array(dim = c(sum(wt1), numparam, numnet))\n      pred <- matrix(nrow = numparam, ncol = numnet)\n      mataux <- sqrt(diag(weights))\n      paramaux <- as.matrix(mataux %*% param[wt1, ])\n      scaledaux <- mataux %*% scaled.sumstat[wt1, ]\n      for (parcur in (1:numparam)) {\n        fit1 <- lm.ridge(paramaux[, parcur] ~ scaledaux, \n                         lambda = lambda)\n        for (i in 1:numnet) {\n          fv[, parcur, i] <- drop(cbind(1, scaled.sumstat[wt1, \n                                                          ]) %*% (rbind(coef(fit1))[i, ]))\n          pred[parcur, i] <- drop(c(1, target) %*% (rbind(coef(fit1))[i, \n                                                                      ]))\n        }\n      }\n      pred.med <- apply(pred, 1, median)\n      pred.med <- matrix(pred.med, nrow = sum(wt1), ncol = numparam, \n                         byrow = T)\n      fitted.values <- apply(fv, c(1, 2), median)\n      residuals <- param[wt1, ] - fitted.values\n      if (hcorr == TRUE) {\n        pred2 <- matrix(nrow = numparam, ncol = numnet)\n        fv2 <- array(dim = c(sum(wt1), numparam, numnet))\n        for (parcur in (1:numparam)) {\n          lresidaux <- (mataux %*% (log(residuals[, parcur]^2)))\n          fit2 <- lm.ridge(lresidaux ~ scaledaux, lambda = lambda)\n          for (i in 1:numnet) {\n            fv2[, parcur, i] <- drop(cbind(1, scaled.sumstat[wt1, \n                                                             ]) %*% (rbind(coef(fit2))[i, ]))\n            pred2[parcur, i] <- drop(c(1, target) %*% \n                                       (rbind(coef(fit2))[i, ]))\n          }\n        }\n        cat(\"\\n\")\n        pred.sd <- sqrt(exp(apply(pred2, 1, median)))\n        pred.sd <- matrix(pred.sd, nrow = sum(wt1), ncol = numparam, \n                          byrow = T)\n        fv.sd <- sqrt(exp(apply(fv2, c(1, 2), median)))\n        adj.values <- pred.med + (pred.sd * residuals)/fv.sd\n        residuals <- (pred.sd * residuals)/fv.sd\n      }\n      else {\n        adj.values <- pred.med + residuals\n      }\n      colnames(adj.values) <- colnames(unadj.values)\n      for (i in 1:numparam) {\n        adj.values[, i] <- adj.values[, i] * param.mad[i]\n      }\n    }\n  }\n  if (numparam == 1) {\n    unadj.values <- matrix(unadj.values, ncol = 1)\n    if (method != \"rejection\") {\n      adj.values <- matrix(adj.values, ncol = 1)\n      residuals <- matrix(residuals, ncol = 1)\n    }\n  }\n  for (i in 1:numparam) {\n    if (transf[i] == \"log\") {\n      unadj.values[, i] <- exp(unadj.values[, i])\n      adj.values[, i] <- exp(adj.values[, i])\n    }\n    else if (transf[i] == \"logit\") {\n      unadj.values[, i] <- exp(unadj.values[, i])/(1 + \n                                                     exp(unadj.values[, i]))\n      unadj.values[, i] <- unadj.values[, i] * (logit.bounds[i, \n                                                             2] - logit.bounds[i, 1]) + logit.bounds[i, 1]\n      adj.values[, i] <- exp(adj.values[, i])/(1 + exp(adj.values[, \n                                                                  i]))\n      adj.values[, i] <- adj.values[, i] * (logit.bounds[i, \n                                                         2] - logit.bounds[i, 1]) + logit.bounds[i, 1]\n    }\n  }\n  abc.return(transf, logit.bounds, method, call, numparam, \n             nss, paramnames, statnames, unadj.values, adj.values, \n             ss, weights, residuals, dist, wt1, gwt, lambda, hcorr, \n             aic, bic)\n}\n\n",
    "created" : 1511388231289.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1087859051",
    "id" : "DCC28713",
    "lastKnownWriteTime" : 1522290346,
    "last_content_update" : 1522290346837,
    "path" : "E:/GIT/paleo-models-sandbox/ABC_Mammoth_CodeV3_KTS.R",
    "project_path" : "ABC_Mammoth_CodeV3_KTS.R",
    "properties" : {
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}